{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8136c449-8445-45fd-8d23-358f6e639e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# note: same thing for REPL\n",
    "# note: we use this instead of magic because `black` will otherwise fail to format\n",
    "#\n",
    "# Enable autoreload to automatically reload modules when they change\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "# do this so that formatter not messed up\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Import commonly used libraries\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# more itertools\n",
    "import more_itertools as mi\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# tensor manipulation\n",
    "# from einops import rearrange, reduce, repeat\n",
    "\n",
    "# automatically apply jaxtyping\n",
    "# %load_ext jaxtyping\n",
    "# %jaxtyping.typechecker typeguard.typechecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33b19509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import openai\n",
    "\n",
    "MarkdownStr = str\n",
    "\n",
    "\n",
    "def concatenate_md_files(root_dir: pathlib.Path) -> MarkdownStr:\n",
    "    md_contents = []\n",
    "\n",
    "    # Combine iterators for .md and .qmd files\n",
    "    md_qmd_files = itertools.chain(root_dir.rglob(\"**/*.md\"), root_dir.rglob(\"**/*.qmd\"))\n",
    "\n",
    "    for filepath in md_qmd_files:\n",
    "        content = filepath.read_text(encoding=\"utf-8\")\n",
    "        md_contents.append(content)\n",
    "\n",
    "    concatenated_content = \"\\n\\n\".join(md_contents)\n",
    "\n",
    "    return concatenated_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79f6159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280256\n"
     ]
    }
   ],
   "source": [
    "root_dir = pathlib.Path(\"inspect_ai/docs\")\n",
    "\n",
    "concatenated_md_files = concatenate_md_files(root_dir)\n",
    "\n",
    "print(len(concatenated_md_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36334c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated markdown files have been stored in: concatenated_md_files.txt\n"
     ]
    }
   ],
   "source": [
    "# Store the concatenated markdown files in a text file so we can sanity check it\n",
    "output_file_path = \"concatenated_md_files.txt\"\n",
    "\n",
    "# Write the concatenated content to the file\n",
    "# Using 'with' ensures the file is properly closed after writing\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(concatenated_md_files)\n",
    "\n",
    "print(f\"Concatenated markdown files have been stored in: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import tqdm\n",
    "\n",
    "# Create a GPT-4o tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# create an openai client\n",
    "openai_client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a2429f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 280256\n",
      "Number of tokens: 63537\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "print(f\"Number of characters: {len(concatenated_md_files)}\")\n",
    "print(f\"Number of tokens: {count_tokens(concatenated_md_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed7705c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cheat_sheet_prompt(concatenated_content: MarkdownStr) -> str:\n",
    "\n",
    "    cheat_sheet_prompt = rf\"\"\"\n",
    "You are tasked with creating a comprehensive yet compact guide based on the markdown documentation of a Python library. This guide will serve as a reference for another AI model when assisting users with the library. Your goal is to distill the essential information into a concise format that captures the key functionality and usage of the library.\n",
    "\n",
    "Here is the markdown documentation for the Python library:\n",
    "\n",
    "<docstring>\n",
    "{concatenated_content}\n",
    "</docstring>\n",
    "\n",
    "Follow these steps to create the guide:\n",
    "\n",
    "1. Carefully read through the entire docstring to understand the structure and content of the library documentation.\n",
    "\n",
    "2. Identify the main components of the library, such as:\n",
    "   - Core classes\n",
    "   - Important functions\n",
    "   - Key modules\n",
    "   - Significant features\n",
    "\n",
    "3. For each main component, extract and summarize the following information:\n",
    "   - Brief description (1-2 sentences)\n",
    "   - Key parameters or attributes\n",
    "   - Important methods (for classes)\n",
    "   - Usage examples (if provided and crucial for understanding)\n",
    "\n",
    "4. Organize the information in a hierarchical structure, starting with the most fundamental elements of the library and progressing to more specific or advanced features.\n",
    "\n",
    "5. Use concise language and avoid redundancy. Aim to capture the essence of each component in as few words as possible while maintaining clarity.\n",
    "\n",
    "6. If the library has a clear workflow or common usage patterns, include a brief overview of these at the beginning of the guide.\n",
    "\n",
    "7. Omit any information related to command-line tools or installation procedures.\n",
    "\n",
    "8. If there are deprecated features or version-specific notes, include only if they are critical for understanding the current functionality of the library.\n",
    "\n",
    "9. Format your guide using markdown syntax for better readability. Use headers (##, ###) to separate sections and subsections.\n",
    "\n",
    "10. Keep the total length of the guide to a few thousand tokens (approximately 2000-3000 words).\n",
    "\n",
    "Structure your output as follows:\n",
    "\n",
    "<guide>\n",
    "## [Library Name] Quick Reference Guide\n",
    "\n",
    "[Brief introduction to the library and its primary purpose]\n",
    "\n",
    "### Core Components\n",
    "[List and briefly describe the main classes, functions, or modules]\n",
    "\n",
    "### Key Functionality\n",
    "[Summarize the most important features and capabilities]\n",
    "\n",
    "### Detailed Component Descriptions\n",
    "[For each major component, provide a concise description, key parameters/methods, and brief usage examples if crucial]\n",
    "\n",
    "### Common Workflows (if applicable)\n",
    "[Outline typical usage patterns or workflows]\n",
    "\n",
    "### Additional Notes\n",
    "[Any critical information that doesn't fit into the above categories]\n",
    "</guide>\n",
    "\n",
    "Remember to focus on creating a guide that is both comprehensive and compact. It needs to contain enough information that another AI model would be able to assist users effectively with this library without additional reference material.\"\"\"\n",
    "\n",
    "    return cheat_sheet_prompt\n",
    "\n",
    "\n",
    "def generate_cheat_sheet(client: openai.OpenAI, concatenated_content: MarkdownStr) -> str:\n",
    "\n",
    "    # Prepare the messages for the Chat Completion API\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI language model that generates comprehensive guides for programming libraries that are used as a reference for larger models.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": generate_cheat_sheet_prompt(concatenated_content),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Call the OpenAI Chat Completion API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=2048,\n",
    "        temperature=1.0,\n",
    "        # n=1,\n",
    "    )\n",
    "\n",
    "    cheat_sheet = response.choices[0].message.content.strip()\n",
    "\n",
    "    print(f\"Generated cheat sheet length (chars): {len(cheat_sheet)}\")\n",
    "    print(f\"Generated cheat sheet length (tokens): {count_tokens(cheat_sheet)}\")\n",
    "    print(f\"Generated cheat sheet: {cheat_sheet}\")\n",
    "\n",
    "    return cheat_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b359e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4d61dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated cheat sheet length (chars): 4491\n",
      "Generated cheat sheet length (tokens): 995\n",
      "Generated cheat sheet: <guide>\n",
      "## Inspect Quick Reference Guide\n",
      "\n",
      "Inspect is a Python framework for evaluating large language models (LLMs) with ease and flexibility. It provides built-in components for dataset handling, solvers, scoring mechanisms, and advanced error management.\n",
      "\n",
      "### Core Components\n",
      "- **Sample**: The main data structure for holding inputs, targets, choices, IDs, and metadata for evaluation tasks.\n",
      "- **Task**: Defines an evaluation task comprising a dataset, solvers to process the inputs, and a scorer to assess the outputs.\n",
      "- **Solver**: A function that produces responses from a model based on the input and may include mechanisms for tool usage, multi-turn interactions, or prompt engineering.\n",
      "- **Scorer**: Evaluates the model's outputs against expected results using various methods, including similarity checks, model scoring, or custom logic.\n",
      "- **Dataset**: Reloading mechanism that supports CSV, JSON, JSON Lines, and integration with external datasets like Hugging Face.\n",
      "\n",
      "### Key Functionality\n",
      "- **Sample Preservation**: Allows re-use of completed samples to save time and costs during evaluation.\n",
      "- **Retry Mechanism**: Resumes tasks when initial evaluation fails without losing progress on successful samples.\n",
      "- **Concurrency Management**: Configures the parallel execution of tasks, model connections, and subprocesses.\n",
      "- **Rich Logging**: Provides detailed evaluation logs and a viewer for interactive exploration of results.\n",
      "\n",
      "### Detailed Component Descriptions\n",
      "\n",
      "#### Sample\n",
      "- **Fields**:\n",
      "  - `input`: The prompt or question for the model.\n",
      "  - `target`: The expected output.\n",
      "  - `choices`: Possible answers (for multiple-choice tasks).\n",
      "  - `metadata`: Additional data related to the sample.\n",
      "- **Usage Example**:\n",
      "    ```python\n",
      "    Sample(input=\"What is 2 + 2?\", target=\"4\", choices=[\"3\", \"4\", \"5\"])\n",
      "    ```\n",
      "\n",
      "#### Task\n",
      "- **Parameters**:\n",
      "  - `dataset`: Dataset object or path to a dataset file.\n",
      "  - `plan`: List of solvers to process the task.\n",
      "  - `scorer`: Scoring mechanism for evaluating the model's output.\n",
      "- **Usage Example**:\n",
      "    ```python\n",
      "    @task\n",
      "    def math_task():\n",
      "        return Task(\n",
      "            dataset=csv_dataset(\"math_data.csv\"),\n",
      "            plan=[generate(), self_critique()],\n",
      "            scorer=match()\n",
      "        )\n",
      "    ```\n",
      "\n",
      "#### Solver\n",
      "- **Types of Solvers**:\n",
      "  - `generate()`: Calls the model to generate outputs based on input prompts.\n",
      "  - `system_message()`: Prepares system prompts to orient the model's responses.\n",
      "  - `self_critique()`: Enables models to review and improve their previous outputs.\n",
      "- **Usage Example**:\n",
      "    ```python\n",
      "    async def custom_solver(state, generate):\n",
      "        # Custom logic\n",
      "        return await generate(state)\n",
      "    ```\n",
      "\n",
      "#### Scorer\n",
      "- **Types**:\n",
      "  - `includes()`: Checks if the target appears in the model's output.\n",
      "  - `match()`: Evaluates if the output matches the target based on specified rules.\n",
      "  - `model_graded_fact()`: Uses a separate model to grade outputs against expected values.\n",
      "- **Usage Example**:\n",
      "    ```python\n",
      "    scorer = model_graded_fact(model=\"openai/gpt-4\")\n",
      "    ```\n",
      "\n",
      "### Common Workflows\n",
      "1. **Defining an Evaluation**:\n",
      "   Write a function annotated with `@task` that returns a `Task`. Define dataset, solvers, and scorer.\n",
      "   ```python\n",
      "   @task\n",
      "   def example_task():\n",
      "       return Task(...)\n",
      "   ```\n",
      "   \n",
      "2. **Running an Evaluation**:\n",
      "   Execute via CLI with `inspect eval <script.py>` or directly call `eval()`.\n",
      "   ```bash\n",
      "   inspect eval example_task.py --model openai/gpt-4\n",
      "   ```\n",
      "\n",
      "3. **Dynamic Evaluation**:\n",
      "   Use parameters to modify the task's behavior. Combine with `itertools.product()` to sweep hyperparameters.\n",
      "   ```python\n",
      "   from itertools import product\n",
      "   for param in product(param1_values, param2_values):\n",
      "       eval(example_task, model=\"openai/gpt-4\", params=param)\n",
      "   ```\n",
      "\n",
      "4. **Error Handling and Retries**:\n",
      "   Use `fail_on_error` in the Task definition to set tolerance levels for errors.\n",
      "   ```python\n",
      "   return Task(fail_on_error=0.1)\n",
      "   ```\n",
      "\n",
      "### Additional Notes\n",
      "- **Caching**: Use the cache feature to save model responses and reduce API calls.\n",
      "- **Sandboxing**: Important for executing tools that perform potentially unsafe operations.\n",
      "- **Eval Logs**: Logs contain comprehensive records of each evaluation, accessible via the log viewer.\n",
      "\n",
      "By leveraging the outlined components and practices, users can efficiently conduct and manage language model evaluations using Inspect. This guide serves as a foundational reference to help users navigate and utilize the library effectively.\n",
      "</guide>\n"
     ]
    }
   ],
   "source": [
    "cheat_sheet = generate_cheat_sheet(openai_client, concatenated_md_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894812d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
